{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b34ff7-38e8-452f-8be4-78675a918faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(25., grad_fn=<SumBackward0>)\n",
      "loss_2 :  tensor([ 8., 17.], grad_fn=<AbsBackward0>)\n",
      "tensor([ 8., 12.]) None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/qgzvcj615dz8g2z95hz3mpfh0000gn/T/ipykernel_4250/2749883694.py:14: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  print(x.grad, y .grad, z.grad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nx=[2,3]을 수식에 대입하면\\n\\n# y=[4,9]\\n\\n# z=[11,21] 이 되는데요, loss = z-target이기에 print out을 하게되면\\n\\n# [8,17]이 나오며, loss.sum = 8+17=25가 됩니다.\\n\\n# loss.backward() 함수를 통해 기울기(gradient)값을 연산하도록 하면, \\n\\n# (Z-target)에 대한 미분값 = Z' = 4x로 [8,12]를 print하게 됩니다.\\n\\n# 여기서 중요한점이 loss.backward()부분입니다.\\n\\n# loss함수가 Z에 대한 함수였기에 [8,12]로 나타내지만, Y에 대한 함수였다면 y'=2x =[4,6]을 print하게 됩니다.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(data = [2.0,3.0], requires_grad = True)\n",
    "y = x**2\n",
    "z = 2*y + 3\n",
    "\n",
    "target = torch.tensor([3.0,4.0])\n",
    "loss = torch.sum(torch.abs(z-target))\n",
    "loss_2 = torch.abs(z-target)\n",
    "loss.backward()\n",
    "\n",
    "print('loss : ',loss)\n",
    "print('loss_2 : ', loss_2)\n",
    "print(x.grad, y .grad, z.grad)\n",
    "\n",
    "'''\n",
    "x=[2,3]을 수식에 대입하면\n",
    "\n",
    "# y=[4,9]\n",
    "\n",
    "# z=[11,21] 이 되는데요, loss = z-target이기에 print out을 하게되면\n",
    "\n",
    "# [8,17]이 나오며, loss.sum = 8+17=25가 됩니다.\n",
    "\n",
    "# loss.backward() 함수를 통해 기울기(gradient)값을 연산하도록 하면, \n",
    "\n",
    "# (Z-target)에 대한 미분값 = Z' = 4x로 [8,12]를 print하게 됩니다.\n",
    "\n",
    "# 여기서 중요한점이 loss.backward()부분입니다.\n",
    "\n",
    "# loss함수가 Z에 대한 함수였기에 [8,12]로 나타내지만, Y에 대한 함수였다면 y'=2x =[4,6]을 print하게 됩니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be31c505-615b-471a-bab3-213d02f86e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(25., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print('loss : ', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "706eaed8-a2b2-4276-a7b2-1c3ee5d97e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_2 :  tensor([ 8., 17.], grad_fn=<AbsBackward0>)\n",
      "tensor([ 8., 12.]) None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jb/qgzvcj615dz8g2z95hz3mpfh0000gn/T/ipykernel_4250/3302990927.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:485.)\n",
      "  print(x.grad, y .grad, z.grad)\n"
     ]
    }
   ],
   "source": [
    "print('loss_2 : ', loss_2)\n",
    "print(x.grad, y .grad, z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c6d68-01d5-4ca5-8b92-a3d96e8151a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
